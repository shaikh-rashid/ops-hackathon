{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Hadoop/Python\n",
    "- - -\n",
    "\n",
    "In this tutorial, students will learn how to use Python with Apache Hadoop to store, process, and analyze incredibly large data sets. Hadoop has become the standard in distributed data processing, but has mostly required Java in the past. Today, there are a numerous open source projects that support Hadoop in Python and this tutorial will show students how to use them.\n",
    "\n",
    "Working with Hadoop using Python instead of Java is entirely possible with a conglomeration of active open source projects that provide Python APIs to Hadoop components. This tutorial will survey the most important projects and show that not only is Hadoop with Python possible, but that it also has some advantages over Hadoop with Java.\n",
    "\n",
    "The reasons for using Hadoop with Python instead of Java are not all that different than the classic Java vs. Python arguments. One of the most important differences is not having to compile your code by instead using a scripting language. This makes more interactive development of analytics possible, makes maintaining and fixing applications in production environments simpler in many cases, makes for more succinct and easier to read code, and so much more. Also, by integrating Python with Hadoop, you get access to the world-class data analysis libraries such as numpy, scipy, nltk, and scikit-learn that are best-in-breed both inside of Python and outside.\n",
    "\n",
    "Students will be surprised at how quickly they can get up and running with Hadoop when using Python. In this tutorial, we will talk about the following libraries and approaches and will guide students through a series of exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (client.py, line 1473)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/python/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2963\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-648040e7f689>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from snakebite.client import Client\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/global/home/ops_srashid/.local/lib/python3.6/site-packages/snakebite/client.py\"\u001b[0;36m, line \u001b[0;32m1473\u001b[0m\n\u001b[0;31m    baseTime = min(time * (1L << retries), cap);\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import PyPDF2\n",
    "from snakebite.client import Client\n",
    "\n",
    "# if using jupyterhub, start a session using the \"Python 3 + PySpark\" kernel\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "# step 1: load file from HDFS\n",
    "hdfs = Client(\"localhost\", 9000, use_trash=False)\n",
    "\n",
    "# get a full object\n",
    "pdf = hdfs.cat(['/user/ops_srashid/origin_of_species.pdf'])\n",
    "\n",
    "# convert pdf to text\n",
    "pdfObject = PyPDF2.PdfFileReader(pdf)\n",
    "wordlist = pdfObject.extractText()\n",
    "hdfs.mkdir('/user/ops_srashid/extract')\n",
    "\n",
    "\n",
    "# step 2: normalize data\n",
    "\n",
    "# replace empty lines, punctuation, and special characters with space\n",
    "# convert all words to lowercase\n",
    "words = re.compile(r'\\W+', re.UNICODE).split(wordlist.lower())\n",
    "# print(sorted(words))\n",
    "\n",
    "\n",
    "# step 3: save normalized, intermediate data\n",
    "\n",
    "# df: dataframe\n",
    "df = pd.Series(sorted(words))\n",
    "# df = np.array(sorted(words))\n",
    "\n",
    "# f: file\n",
    "f = '/tmp/normalized.pkl'\n",
    "\n",
    "df.to_pickle(f)\n",
    "\n",
    "# fd: file descriptor\n",
    "fd = os.stat(f)\n",
    "\n",
    "\n",
    "# step 4: create dictionary of words and frequencies\n",
    "wordcount = Counter(df)\n",
    "\n",
    "# quick statistic summary of your data\n",
    "# print(wordcount.describe())\n",
    "\n",
    "\n",
    "# step 5: output results\n",
    "# print(wordcount)\n",
    "# print(wordcount.index)\n",
    "# print(wordcount.columns)\n",
    "# print(wordcount.values)\n",
    "# print(wordcount.sort_index(axis=1, ascending=False))\n",
    "print(wordcount.most_common(10))\n",
    "\n",
    "# step 6: do the same thing... but in Spark\n",
    "# connect to spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster('yarn-client')\n",
    "conf.set('spark.executor.memory', \"5g\")\n",
    "conf.set('spark.driver.memory', \"500m\")\n",
    "conf.set('spark.yarn.dist.files','file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip,file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip')\n",
    "conf.setExecutorEnv('PYTHONPATH','pyspark.zip:py4j-0.10.6-src.zip')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# do spark work here\n",
    "text_file = sc.textFile(\"hdfs://user/ops_srashid/extract/\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "# disconnect from spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Pyspark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
